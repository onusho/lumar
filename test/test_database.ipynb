{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import weaviate\n",
    "from weaviate.classes.config import Configure, Multi2VecField\n",
    "from weaviate.classes.query import Filter\n",
    "from loader import DataLoader\n",
    "import base64\n",
    "from hashlib import md5\n",
    "\n",
    "class DatabaseClient:\n",
    "    \n",
    "    def __init__(self, folder_path):\n",
    "        self.__folder_path = folder_path\n",
    "        self.__create_client()\n",
    "        if self.__generate_collection():\n",
    "            self.loader = DataLoader(folder_path, chunk_size=300, chunk_overlap=50)\n",
    "            self.__data_ingestion(folder_path)\n",
    "   \n",
    "    def __create_client(self):\n",
    "        self.client = weaviate.connect_to_local()\n",
    "    \n",
    "    def __get_hashed_path(self):\n",
    "        return self.__folder_path.split(\"\\\\\")[-1]  + ''.join(filter(str.isalpha, md5(self.__folder_path.encode()).hexdigest()))\n",
    "\n",
    "    def __generate_collection(self):\n",
    "        collection_name = self.__get_hashed_path()\n",
    "        if self.client.collections.exists(collection_name):\n",
    "            # self.collection = self.client.collections.get(self.__get_hashed_path())\n",
    "            # return False\n",
    "            self.client.collections.delete(collection_name)\n",
    "\n",
    "        self.collection = self.client.collections.create(\n",
    "            name=collection_name,\n",
    "            vectorizer_config=Configure.Vectorizer.text2vec_ollama(\n",
    "                api_endpoint=\"http://host.docker.internal:11434\",\n",
    "                model='nomic-embed-text'\n",
    "            )\n",
    "        )\n",
    "        return True\n",
    "    \n",
    "    def __data_ingestion(self, folder_path):\n",
    "        object_list = self.loader.load_data()\n",
    "        with self.collection.batch.dynamic() as batch:\n",
    "            for object in object_list:\n",
    "                batch.add_object(\n",
    "                    properties=object\n",
    "                )\n",
    "    \n",
    "    def search_with_text(self, query : str, search_for=\"all\"):\n",
    "        if search_for == \"all\":\n",
    "            response =  self.collection.query.near_text(\n",
    "                query=query,\n",
    "                limit=5\n",
    "            )\n",
    "        elif search_for == \"image\":\n",
    "            response =  self.collection.query.near_text(\n",
    "                query=query,\n",
    "                filters=Filter.by_property(\"media_type\").equal(\"image\"),\n",
    "                limit=5\n",
    "            )\n",
    "        elif search_for == \"text\":\n",
    "            response =  self.collection.query.near_text(\n",
    "                query=query,\n",
    "                filters=Filter.by_property(\"media_type\").equal(\"text\"),\n",
    "                limit=5\n",
    "            )\n",
    "\n",
    "        return [object.properties for object in response.objects]\n",
    "        \n",
    "    def search_with_image(self, image_path, search_for = 'all'):\n",
    "        def to_base64(path):\n",
    "            with open(path, 'rb') as file:\n",
    "                return base64.b64encode(file.read()).decode('utf-8')\n",
    "        if search_for == \"all\":\n",
    "            response = self.collection.query.near_image(\n",
    "                near_image=to_base64(image_path),\n",
    "                limit=5\n",
    "            )\n",
    "        elif search_for == \"image\":\n",
    "            response = self.collection.query.near_image(\n",
    "                near_image=to_base64(image_path),\n",
    "                filters=Filter.by_property(\"media_type\").equal(\"image\"),\n",
    "                limit=5\n",
    "                )\n",
    "        elif search_for == \"text\":\n",
    "            response = self.collection.query.near_image(\n",
    "                near_image=to_base64(image_path),\n",
    "                filters=Filter.by_property(\"media_type\").equal(\"text\"),\n",
    "                limit=5\n",
    "            )\n",
    "        return [object.properties for object in response.objects]\n",
    "    \n",
    "    def list_collections(self):\n",
    "        return self.client.collections.list_all()\n",
    "    \n",
    "    def delete_collections(self, name):\n",
    "        return self.client.collections.delete(name)\n",
    "    \n",
    "    def close_connection(self):\n",
    "        self.client.close()\n",
    "            \n",
    "    def __repr__(self):\n",
    "        return f\"\"\"Folder: {self.__folder_path}\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "database = DatabaseClient(folder_path=r\"C:\\Users\\Anush\\Desktop\\Christ\\Specialization Project\\Localinsight\\text-only-test\\documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Documentseebedcaceeab': _CollectionConfigSimple(name='Documentseebedcaceeab', description=None, generative_config=None, properties=[_Property(name='path', description=\"This property was generated by Weaviate's auto-schema feature on Thu Aug 22 05:49:04 2024\", data_type=<DataType.TEXT: 'text'>, index_filterable=True, index_range_filters=False, index_searchable=True, nested_properties=None, tokenization=<Tokenization.WORD: 'word'>, vectorizer_config=_PropertyVectorizerConfig(skip=False, vectorize_property_name=False), vectorizer='text2vec-ollama'), _Property(name='media_type', description=\"This property was generated by Weaviate's auto-schema feature on Thu Aug 22 05:49:04 2024\", data_type=<DataType.TEXT: 'text'>, index_filterable=True, index_range_filters=False, index_searchable=True, nested_properties=None, tokenization=<Tokenization.WORD: 'word'>, vectorizer_config=_PropertyVectorizerConfig(skip=False, vectorize_property_name=False), vectorizer='text2vec-ollama'), _Property(name='text', description=\"This property was generated by Weaviate's auto-schema feature on Thu Aug 22 05:49:04 2024\", data_type=<DataType.TEXT: 'text'>, index_filterable=True, index_range_filters=False, index_searchable=True, nested_properties=None, tokenization=<Tokenization.WORD: 'word'>, vectorizer_config=_PropertyVectorizerConfig(skip=False, vectorize_property_name=False), vectorizer='text2vec-ollama'), _Property(name='chunk_no', description=\"This property was generated by Weaviate's auto-schema feature on Thu Aug 22 05:49:04 2024\", data_type=<DataType.NUMBER: 'number'>, index_filterable=True, index_range_filters=False, index_searchable=False, nested_properties=None, tokenization=None, vectorizer_config=_PropertyVectorizerConfig(skip=False, vectorize_property_name=False), vectorizer='text2vec-ollama')], references=[], reranker_config=None, vectorizer_config=_VectorizerConfig(vectorizer=<Vectorizers.TEXT2VEC_OLLAMA: 'text2vec-ollama'>, model={'apiEndpoint': 'http://host.docker.internal:11434', 'model': 'nomic-embed-text'}, vectorize_collection_name=True), vectorizer=<Vectorizers.TEXT2VEC_OLLAMA: 'text2vec-ollama'>, vector_config=None)}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "database.list_collections()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Why did Alice decide to follow the White Rabbit down the rabbit-hole?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': 'Alice was not a bit hurt, and she jumped up on to her feet in a moment: she looked up, but it was all dark overhead; before her was another long passage, and the White Rabbit was still in sight, hurrying down it. There was not a moment to be lost: away went Alice like the wind, and was just in time',\n",
       "  'chunk_no': 23.0,\n",
       "  'path': 'C:\\\\Users\\\\Anush\\\\Desktop\\\\Christ\\\\Specialization Project\\\\Localinsight\\\\text-only-test\\\\documents\\\\alice.txt',\n",
       "  'media_type': 'text'},\n",
       " {'text': 'So she was considering in her own mind (as well as she could, for the hot day made her feel very sleepy and stupid), whether the pleasure of making a daisy-chain would be worth the trouble of getting up and picking the daisies, when suddenly a White Rabbit with pink eyes ran close by her.',\n",
       "  'chunk_no': 3.0,\n",
       "  'path': 'C:\\\\Users\\\\Anush\\\\Desktop\\\\Christ\\\\Specialization Project\\\\Localinsight\\\\text-only-test\\\\documents\\\\alice.txt',\n",
       "  'media_type': 'text'},\n",
       " {'text': 'she had never before seen a rabbit with either a waistcoat-pocket, or a watch to take out of it, and burning with curiosity, she ran across the field after it, and fortunately was just in time to see it pop down a large rabbit-hole under the hedge.',\n",
       "  'chunk_no': 6.0,\n",
       "  'path': 'C:\\\\Users\\\\Anush\\\\Desktop\\\\Christ\\\\Specialization Project\\\\Localinsight\\\\text-only-test\\\\documents\\\\alice.txt',\n",
       "  'media_type': 'text'},\n",
       " {'text': 'After a time she heard a little pattering of feet in the distance, and she hastily dried her eyes to see what was coming. It was the White Rabbit returning, splendidly dressed, with a pair of white kid gloves in one hand and a large fan in the other: he came trotting along in a great hurry,',\n",
       "  'media_type': 'text',\n",
       "  'path': 'C:\\\\Users\\\\Anush\\\\Desktop\\\\Christ\\\\Specialization Project\\\\Localinsight\\\\text-only-test\\\\documents\\\\alice.txt',\n",
       "  'chunk_no': 63.0},\n",
       " {'text': 'other: he came trotting along in a great hurry, muttering to himself as he came, “Oh! the Duchess, the Duchess! Oh! won’t she be savage if I’ve kept her waiting!” Alice felt so desperate that she was ready to ask help of any one; so, when the Rabbit came near her, she began, in a low, timid voice,',\n",
       "  'chunk_no': 64.0,\n",
       "  'path': 'C:\\\\Users\\\\Anush\\\\Desktop\\\\Christ\\\\Specialization Project\\\\Localinsight\\\\text-only-test\\\\documents\\\\alice.txt',\n",
       "  'media_type': 'text'}]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "database.search_with_text(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
